{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function: It takes in z = w*x and if actf(z)> threshold(theta) then 1 and -1 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for perceptron activation function is unit step, i.e. actf(z) = 1 if z>theta and actfz(z) = -1 if z < theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Perceptron rule:   w_change = ita*(y_true - y_pred)*x\n",
    "After w_change is calculated , weights are updated w_new = w_change + w_old\n",
    "where ita is learning rate generally between 0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small otherwise weights keep getting updated. To avoid this , run the perceptron for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ADALINE: Adaptive Linear Neuron\n",
    "\n",
    "Uses identity function as activation function.  actf(w_transpo*x) = w_transpo*x\n",
    "\n",
    "and a step function for classification (quantizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the case of Adaline, we can define the cost function J to learn the weights as the Sum of Squared Errors (SSE) between the calculated outcome and the true class label.\n",
    "\n",
    "J is differentiable and convex thus \"Gradient descent\" a simple yet powerful optimization algo can be\n",
    "used to get weights minimizing the cost function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choosing a Classification algo\n",
    "\n",
    "\"No Free Lunch\" theorem: no single classifier works best across all possible scenarios. In practice, \n",
    "\n",
    "### it is always recommended that you compare the performance of at least a handful of different learning algorithms to select the best model for the particular problem; \n",
    "\n",
    "### ## Once you have decided with complexity, train with all the training data so that you do not miss valuable data.\n",
    "\n",
    "these may differ in the \n",
    "\n",
    "1. number of features or samples,\n",
    "\n",
    "2. the amount of noise in a dataset, and \n",
    "\n",
    "3. whether the classes are linearly separable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Steps to solve a Machine learning problem:\n",
    " \n",
    " 1. Selection of features. \n",
    " \n",
    " 2. Choosing a performance metric. \n",
    " \n",
    " 3. Choosing a classifier and optimization algorithm. \n",
    " \n",
    " 4. Evaluating the performance of the model. \n",
    " \n",
    " 5. Tuning the algorithm.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling features\n",
    "\n",
    "##  Use the same scaling parameters to standardize the test set so that both the values in the training and test dataset are comparable to each other. Use the same scaler that was fit using training data.\n",
    "\n",
    "## Rescaling the features such that they are on the same scale and have equal influence on the results.\n",
    "X' = (X - Xmin)/(Xmax - Xmin)\n",
    "\n",
    "## Before scaling REMOVE THE OUTLIERS coz OUTLIERS WILL MESS WITH SCALING.\n",
    "sklearn's minmaxscaler\n",
    "\n",
    "### Algorithms which involve two or more dimensions will be affected by feature scaling.\n",
    " But since in regression features go with coefficients which take care of scale of that feature. \n",
    " \n",
    " Also in decision trees decision boundaries are always vertical or horizontal rendering it unaffected by the size of different features. \n",
    " \n",
    " Kmeans and SVMs will be affected by scaling coz distance calculation is involved with different ### dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "It is a classification algorithm despite the name. \n",
    "\n",
    "A linear model, used for binary classification, can be extended to multiclass classificatioin using OVR(one vs rest) technique.\n",
    "\n",
    "Probabalistic model:\n",
    "\n",
    "Odds ratio = p/(1-p) , \n",
    "where p is probability of the even happening. \n",
    "\n",
    "Logistic regression is used in weather forecasting, for example, to not only predict if it will rain on a particular day but also to report the chance of rain. Similarly, logistic regression can be used to predict the chance that a patient has a particular disease given certain symptoms, which is why logistic regression enjoys wide popularity in the field of medicine.\n",
    "\n",
    "\n",
    "logit(p) = log(p/(1-p))\n",
    "\n",
    "z = w_transpose . x(input)\n",
    "\n",
    "f(z) = Logistic function = 1/(1 + e^(-z)) = Probability of event occuring = p\n",
    "\n",
    "1/(1+e^(-z)) is called sigmoid function because of the s shape it has.\n",
    "\n",
    "for given weights and x, z = w_transpose. x can be found and using sigmoid function and z probability is found. \n",
    "\n",
    "Logistic regression can be used to predict probabilities and class labels.\n",
    "\n",
    "For updating weights\n",
    "### Maximizing the log likelihood = Minimizing the cost function \n",
    "\n",
    "## ∆w = −eta. ∇J(w )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Bayes Rule: \n",
    "\n",
    "P(c) = Probability of cancer(event occuring). (Prior Probability)\n",
    "\n",
    "Test evidence:\n",
    "P(pos/c) = Probability that test is positive when cancer is there. -- Sensitivity (SENACPO)\n",
    "P(pos/-c) = Probability that test is negative when cancer is not there. -- Specificity (SPACNE)\n",
    "\n",
    "Remember:\n",
    "\n",
    "SENACPO -- Number of times we are correct when ACtual value is POsitive.\n",
    "SPACNE  -- Number of times we are correct when ACtual value is NEgative. \n",
    "\n",
    "BAYES RULE:\n",
    "\n",
    "(PRIOR PROB) . (TEST EVIDENCE) -> (POSTERIOR PROB)\n",
    "\n",
    "P(c) = 0.01 (1%)  |  P(-c) = 0.99 (99%)\n",
    "\n",
    "SENSITIVITY: P(pos/c) = 0.9 (90%) | P(neg/c) = 0.1 (10%)\n",
    "\n",
    "SPECIFICITY: P(neg/-c) = 0.9 (90%)| P(pos/-c) = 0.1 (10%) \n",
    "\n",
    "\n",
    "JOINT PROB :\n",
    "\n",
    "P(c/pos) = P(c) . P(pos/c) = 0.01 x .90 = 0.009\n",
    "\n",
    "P(-c/pos) = P(-c) . P(pos/-c) = 0.99 x 0.1 = 0.099\n",
    "\n",
    "\n",
    "joint Probabilities generally dont add up to 1. Normalize them to make them add upto 1. \n",
    "\n",
    "(Normalizer)factor = P(c/pos) + P(-c/pos) = 0.108\n",
    "\n",
    "ACTUAL POSTERIOR PROB:\n",
    "\n",
    "P(c/pos) = 0.009/factor = 0.08333 (8.33%)\n",
    "\n",
    "P(-c/pos) = 0.099/factor = 0.9166 (91.67%)\n",
    "\n",
    "P(c/pos) + P(-c/pos) = 0.0833 + 0.9166 = 1.0\n",
    "\n",
    "\n",
    "##  Uses of Naive Bayes:\n",
    "    \n",
    "### It is used a lot for text learning. \n",
    "\n",
    "### Why Naive? \n",
    "\n",
    "Ignores word order, only considers the frequency of words.\n",
    "\n",
    "### Strengths and Weaknesses\n",
    "\n",
    "Strengths:\n",
    "1. Can handle lots of words (features) 20k and more.\n",
    "2. Easy to implement.\n",
    "\n",
    "Weakness:\n",
    "1. Ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overfitting: High Variance: Can't generalize \n",
    "\n",
    "Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data).\n",
    "\n",
    "If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex given the underlying data. \n",
    "\n",
    "# Underfitting: High Bias: Highly susceptible to data. \n",
    "\n",
    "means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.\n",
    "\n",
    "## Variance: Very Different results for different dataset\n",
    "\n",
    "## Bias: Far from correct values. \n",
    "\n",
    "\n",
    "# Bias Variance Trade-off\n",
    "\n",
    "Bias: \n",
    "\n",
    "1. A high bias ml algo doesn't learn anything from data, practically ignores it. \n",
    "2. Pays little attention to data.\n",
    "3. High error on training set (low Rsquared, high SSE)\n",
    "4. Oversimplified\n",
    "\n",
    "Variance: Willingness and flexibility of an algo to learn. \n",
    "\n",
    "1. A high variance ml algo is highly susceptive to data and can't generalize. \n",
    "2. Memorizes the data.\n",
    "3. Fails to generalize well.\n",
    "4. Much Higher error on testing set (low Rsquared, high SSE)\n",
    "5. Overfitting\n",
    "\n",
    "In stats : Variance means spread of a data distribution.\n",
    "\n",
    "## Underfit -High bias ----> Good Model ----> Overfit High Variance\n",
    "## Finding the optimal number of features for the good model which balances bias and variance. Can be done by:\n",
    "\n",
    "1. Regularization: Penalizes for extra features.\n",
    "\n",
    "# Tackling overfitting via regularization\n",
    "\n",
    "Regularization is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights. The most common form of regularization is the so-called L2 regularization (sometimes also called L2 shrinkage or weight decay\n",
    "\n",
    "# C = 1/λ\n",
    "\n",
    "where, \n",
    "λ = Regularization Strength = More the lambda lesser the weights.\n",
    "C  = Inverse regularization parameter (in Logistic Regression sklearn).\n",
    "By tuning C we can control regularization of in Logistic Regression. \n",
    "\n",
    "# More the C more the weights.\n",
    "# As C decreases weights shrink.\n",
    "\n",
    "# L1 Regularization: linear sum(|w|) --> encourages sparsity, diamond shaped.\n",
    "# L2 Regularization: quadratic sum(w^2) --> (circular)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Feature scaling\n",
    "\n",
    "Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Support Vector Machines\n",
    "\n",
    "Finding a HyPlane(plane separating classes) which is at max distance from nearest samples(support vectors)\n",
    "Maximizing margin between Hyperplane and support vectors(samples closest to HyPlane)\n",
    "\n",
    "Bias-Variance can be controlled by C : Higher the C more penalty for misclassifications lesser variance and more the bias.\n",
    "### Lower C lesser penalties softer margins.\n",
    "\n",
    "Kernel trick: Can be used to tap into high dimensional space to find a linear separating plane in the high dimension where the data seems non linear in regular dimension. Projecting non linear combination on features to higher dimensional space. \n",
    "\n",
    "Roughly speaking, the term kernel can be interpreted as a similarity function between a pair of samples.\n",
    "\n",
    "Gamma = Influence on a sample on nearby region. More the gamma less the influence.\n",
    "\n",
    "## SVMs are hard to scale,\n",
    "\n",
    "# SVM - Support Vector Machine\n",
    "\n",
    "Finds a hyperplane/line(2d) which separates the classes being at max distance from the nearest datapoints of the classes. \n",
    "\n",
    "The distance at which hyplane or line is from the nearest point is called MARGIN.\n",
    "\n",
    "Best Hyperplane/line is the one which maximizes the margin from classes and has most correct classifications. Priority of SVM is correct classification then margin. \n",
    "\n",
    "Tolerates outliers easily. Robust to outliers.\n",
    "\n",
    "## Non Linear SVMs\n",
    "\n",
    "Adding a feature from mathematical combination of existing features e.g z = x^2 + y^2 or z = |x| finds a hyperplane where it's impossible to separate classes linearly using original features.\n",
    "\n",
    "Uses kernels to tap high dimensional space to convert non linearly separable variables in low dimension, finds a hyperplane in high dimension and returns the solution to lower dimension in form of a non linear separator.\n",
    "\n",
    "### Parameters in SVM\n",
    "\n",
    "Kernel = rbf, sigmoid, poly , custom, linear etc.\n",
    "\n",
    "Gamma = radial influence of single data point low gamma meaning far influence, high gamma meaning close influence. \n",
    "The 'gamma' parameter actually has no effect on the 'linear' kernel for SVMs. The key parameter for this kernel function is \"C\".\n",
    "\n",
    "C = Controls the tradeoff between simple decision boundary and correctly classifying training points.\n",
    "Larger the C more the correct classifications, lower the C simpler the decision boundary.\n",
    "\n",
    "Overfitting can be controlled by parameters of the algo, for example in case of SVMs C, Gamma, Kernel.\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "Memory efficient as uses only subset of training points.\n",
    "Performs well in high dimensional data.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "Doesn't perform well with lots and lots of data as order is n^3.\n",
    "Doesn't work well with lots of noise.\n",
    "Very slow compared to Naive Bayes.\n",
    "\n",
    "\n",
    "# SVM tips:\n",
    "\n",
    "1. Changing kernel can improve accuracy drastically eg. rbf - 48% to linear - 97%.\n",
    "2. Reducing sample size increases training and prediction speed but reduces testing accuracy.\n",
    "3. SVMs do not scale well. (O(n) = n^2 , quadratic order)\n",
    "4. Optimized rbf 99% , linear 97%\n",
    "\n",
    "\n",
    "## SGDClassifier can be used for efficient scaling to very large datasets. \n",
    "\n",
    "Different algos from SGDClassifier.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "ppn = SGDClassifier(loss='perceptron') \n",
    "\n",
    "lr = SGDClassifier(loss='log') \n",
    "\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "\n",
    "http://scikit-learn.org/stable/modules/sgd.html \n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "Maximizing information gain at each node. \n",
    "\n",
    "Commonly used splitting criteria(Measures of impurity):\n",
    "\n",
    "1. Gini index -  Ig = 1 - ∑p(i/t)\n",
    "\n",
    "2. Entropy - Ie = -∑p(i/t)log<sub>2</sub>(p(i/t)\n",
    "\n",
    "3. Classification error - Ie =  1 - max(p(i/t))\n",
    "\n",
    "Here, (p(i/t) is the proportion of the samples that belongs to class c for a particular node t. \n",
    "\n",
    "in practice both the Gini index and entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.\n",
    "\n",
    "Classification error is useful for pruning but not for growing tree. \n",
    "\n",
    "    \n",
    "# Decision Trees\n",
    "\n",
    "creates linear decision boundaries.\n",
    "\n",
    "# Parameters\n",
    "\n",
    "min_samples_split = 2(Default)\n",
    "Means won't split if samples at a node < min_sample_split \n",
    "\n",
    "More the min_samples_split, lesser the splits, lesser the complexity, lesser the overfitting.\n",
    "\n",
    "# Entropy - \n",
    "## Measure of impurity in a bunch of examples.\n",
    "\n",
    "Purity: Having all examples of the same class in a splitted section. \n",
    "\n",
    "Entropy/impurity: Having more than 1 examples of other class at a node.\n",
    "\n",
    "Entropy is defined for a node. A node might have multiple classes and thus entropy\n",
    "if a node has only one class, it is a pure node and entropy is 0. \n",
    "\n",
    "Entropy = 1.0 when examples are evenly split amongst classes.\n",
    "Entropy = 0 when only one class is present in a split. Pure !\n",
    "\n",
    "Objective: Minimizing impurity in splitting.\n",
    "\n",
    "Entropy = −∑\n",
    "​i\n",
    "​​ (p\n",
    "​i\n",
    "​​ )log\n",
    "​2\n",
    "​​ (p\n",
    "​i\n",
    "​​ )\n",
    "\n",
    "where, i is a class and pi is % of that class in the split. \n",
    "\n",
    "## Information Gain\n",
    "\n",
    "Gain = entropy(parent) - [weighted average]entropy(children)\n",
    "\n",
    "[weighted average] is calculated basis proportion of samples going in a split.\n",
    "exam 2/3 and 1/3\n",
    "\n",
    "More Gain, lesser entropy in children, more purity, better classification. Objective: Maximize Gain.\n",
    "Decision trees maximize gain.\n",
    "\n",
    "## Gini and Entropy:\n",
    "sklearn has two criterion namely gini and entropy. Default is gini. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decision Tree :\n",
    "Strengths: Can make bigger classifiers(Ensembled Methods).\n",
    "\n",
    "Weakness: Overfitting. (Be careful about parameter tuning). \n",
    "\n",
    "# Reducing complexity of algorithms and improving speed\n",
    "\n",
    "1. Tune parameters\n",
    "2. Identify necessary features and only use them for building model. \n",
    "(Generally more features the algo has, the more complex it is for fitting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Entropy calculator\n",
    "from math import log2\n",
    "def entrocalc(class_samples):\n",
    "    entropy = 0\n",
    "    tot_samples = 0\n",
    "    for val in class_samples:\n",
    "        tot_samples += class_samples[val]\n",
    "    \n",
    "    for key in class_samples:\n",
    "        pi = class_samples[key]/(tot_samples)\n",
    "        entropy = entropy - (pi)*log2(pi)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forests -  Combining weak learners to Strong ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly choose n samples from training set.\n",
    "2. Grow a decision tree select d random features at each node.\n",
    "3. Repeat steps 1 and 2 , k times.\n",
    "4. Aggregate the prediction by each tree to assign a class label through majority vote.\n",
    "\n",
    "A resonable default for  d is sqrt(m) where m is nof features in training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind ensemble learning is to combine weak learners to build a more robust model, a strong learner, that has a better generalization error and is less susceptible to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a big advantage of random forests is that we don't have to worry so much about choosing good hyperparameter values. We typically don't need to prune the random forest since the ensemble model is quite robust to noise from the individual decision trees. The only parameter that we really need to care about in practice is the number of trees k (step 3) that we choose for the random forest. Typically, the larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost. No need to standardize or normalize tree based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors\n",
    "\n",
    "Finds k closest neighbors using a distance metric e.g euclidean distance, manhattan distance and assigns class label by majority vote.  KNN learns training data. Simple but requires lot of memory and not scalable. Complexity grows linearly. \n",
    "\n",
    "## The curse of dimensionality\n",
    "It is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. Intuitively, we can think of even the closest neighbors being too far away in a high-dimensional space to give a good estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of classification algorithms\n",
    "\n",
    "We have seen that decision trees are particularly attractive if we care about interpretability. Logistic regression is not only a useful model for online learning via stochastic gradient descent, but also allows us to predict the probability of a particular event. Although support vector machines are powerful linear models that can be extended to nonlinear problems via the kernel trick, they have many parameters that have to be tuned in order to make good predictions. In contrast, ensemble methods such as random forests don't require much parameter tuning and don't overfit so easily as decision trees, which makes it an attractive model for many practical problem domains. The K-nearest neighbor classifier offers an alternative approach to classification via lazy learning that allows us to make predictions without any model training but with a more computationally expensive prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Better Results \n",
    "(Generally) better than even a super optimized algo.\n",
    "\n",
    "# Always visualize your data. \n",
    "\n",
    "\n",
    "# Outliers\n",
    "\n",
    "### Rare data points which don't follow the trend.\n",
    "\n",
    "Causes:\n",
    "1. Sensor Malfunction - to be ignored\n",
    "2. Data entry errors - to be ignored  \n",
    "3. Freak events: - to be paid attention to.  e.g. Fraud detection\n",
    "\n",
    "Removal:\n",
    "\n",
    "1. Train > 2.Remove(10% with max residual error) > 3.Train again\n",
    "\n",
    "(Repeat steps 2 & three until satisfied)\n",
    "\n",
    "# Visualization is one of the most powerful tools for finding outliers!\n",
    "\n",
    "# First thing to do is to Identify and Clean the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Data\n",
    "\n",
    "1. Numerical: Numbers like 234, 453.0 etc ex. age, height, score.\n",
    "\n",
    "2. Categorical: Discrete values like gender, color, material, job title etc\n",
    "\n",
    "3. TimeSeries: Temporal data (timestamp)\n",
    "\n",
    "4. Text: Words\n",
    "\n",
    "\n",
    "# Be very careful about introducing features that come from different sources depending on the class! It’s a classic way to accidentally introduce biases and mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn. Therefore, it is absolutely critical that we make sure to examine and preprocess a dataset before we feed it to a learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although scikit-learn was developed for working with NumPy arrays, it can sometimes be more convenient to preprocess data using pandas' DataFrame. We can always access the underlying NumPy array of the DataFrame via the values attribute before we feed it into a scikit-learn estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding sklearn estimator\n",
    "\n",
    "## Any data array that is to be transformed needs to have the same number of features as the data array that was used to fit the model.\n",
    "\n",
    "## The fit method is used to learn the parameters from the training data, and the transform method uses those parameters to transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sklearn Estimator](sklearnestimator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Handling missing values \n",
    "\n",
    "1. dropping\n",
    "2. Imputing\n",
    "\n",
    "## Handling categorical data\n",
    "\n",
    "1. LabelEncoding class labels.\n",
    "2. Mapping nominal features to numbers \n",
    "3. One hot encoding nominal features - Getting dummies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing sets\n",
    "\n",
    "## Bringing features onto the same scale.\n",
    "\n",
    "Normalization: bringing data between [0,1]\n",
    "\n",
    "Standardization: standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting \n",
    "\n",
    "occurs when our model performs very well on training set but fails to genralize well on testing sets.  It means our model is too complex for the given data. Overfitting can be reduced by:\n",
    "\n",
    "1. Using less but important features\n",
    "2. Collecting more training data\n",
    "3. Introduce a penalty for complexity via regularization (explained above)\n",
    "4. Reduce dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "There are two main categories of dimensionality reduction techniques: feature selection and feature extraction.\n",
    "\n",
    "Feature selection: we select a subset of the original features. \n",
    "Feature extraction: we derive information from the feature set to construct a new feature subspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Feature Selection \n",
    "\n",
    "Feature selection: selecting a subset of original features. \n",
    "\n",
    "The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization.\n",
    "\n",
    "## Sequential Backward Selection(SBS)\n",
    "\n",
    "Removing features in stages, at each stage a feature that causes minimal accuracy loss (accuracy_before - accuracy_after) is removed. \n",
    "\n",
    "# Feature removal throught regularization (l1 and l2)\n",
    "\n",
    "Penalizing weights. \n",
    "\n",
    "# Feature Importances \n",
    "\n",
    "## Random forest feature importance\n",
    "\n",
    "feature importance as the averaged impurity decrease computed from all decision trees in the forest without making any assumptions whether our data is linearly separable or not. we don't need to use standardized or normalized tree-based models\n",
    "\n",
    "scikit-learn also implements a transform method that selects features based on a user-specified threshold after model fitting, which is useful if we want to use the RandomForestClassifier as a feature selector and intermediate step in a scikit-learn pipeline, which allows us to connect different preprocessing steps with an estimator, as we will see in Chapter 6, Learning Best Practices for Model Evaluation and Hyperparameter Tuning. For example, we could set the threshold to 0.15 to reduce the dataset to the 3 most important features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Feature extraction\n",
    "\n",
    "PCA: Principal component analysis\n",
    "Data is transformed into a low/equal dimensional feature subspace such that principal components(directions of maximum variance) are\n",
    "orthogonal to each other. \n",
    "\n",
    "PCA can be used for:\n",
    "1. Feature extraction(unsupervised learning)\n",
    "2. De noising of signals\n",
    "3. Exploratory data analysis\n",
    "4. Analysis of genome data and gene expression\n",
    "\n",
    "PCA helps us to identify patterns in data based on the correlation between features\n",
    "\n",
    "PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other.\n",
    "\n",
    "![Principal components](pca.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Standardize features before PCA\n",
    "\n",
    "### Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Steps involved in PCA`\n",
    "\n",
    "1. Standardize the d -dimensional dataset. \n",
    "2. Construct the covariance matrix.  (The symmetric d d× -dimensional covariance matrix, where d is the number of dimensions in the dataset, stores the pairwise covariances between the different features.)\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Select k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k d≤ ).\n",
    "5. Construct a projection matrix W from the \"top\" k eigenvectors.\n",
    "6. Transform the d -dimensional input dataset X using the projection matrix W to obtain the new k -dimensional feature subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Covariance\n",
    "\n",
    "A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions.\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. \n",
    "\n",
    "### Eigen values and Eigen vectors\n",
    "\n",
    "From a NxN covariance matrix we get N eigen vectors, eigen values are their magnitudes. \n",
    "We only select the subset of the eigenvectors (principal components) that contains most of the information (variance).\n",
    "\n",
    "We are interested in the top k eigenvectors based on the values of their corresponding eigenvalues.\n",
    "\n",
    "The variance explained ratio of an eigenvalue λ is simply the fraction of an eigenvalue λ and the total sum of the eigenvalues.\n",
    "\n",
    "## PCA is an unsupervised method, which means that information about the class labels is ignored.\n",
    "\n",
    "# PCA\n",
    "principal component analysis\n",
    "\n",
    "Finds a new coordinate system by shift-rotation of current one to reduce dimensionality.\n",
    "\n",
    "New center is the middle point of old data range and principal axis is the one having significant variation.\n",
    "\n",
    "Gives importance vectors\n",
    "\\\n",
    "Gives spread\n",
    "\n",
    "art of the beauty of PCA is that the data doesn't have to be perfectly 1D in order to find the principal axis!\n",
    "\n",
    "Making composite features using PCA to dimension reduction.\n",
    "\n",
    "In stats : Variance means spread of a data distribution.\n",
    "\n",
    "Principal component direction is the one that has maximum variance(spread). Because only in that direction, information loss is minimized.\n",
    "\n",
    "More the distance of data point from principal component more the information loss. \n",
    "\n",
    "## PCA transforms features into principal components.\n",
    "\n",
    "## Principal components are used as new features.\n",
    "\n",
    "## Principal components are perpendicular to each other thus are independent. \n",
    "\n",
    "## Max nof PCs = Nof features\n",
    "\n",
    "\n",
    "## When to use PCA.\n",
    "\n",
    "1. Identifying latent features driving the patterns in the data.\n",
    "2. Dimensionality Reduction.\n",
    "\n",
    "    a. Visualizing High Dimensional data.\n",
    "    \n",
    "    b. Reduce Noise\n",
    "    \n",
    "    c. Make algos work better with fewer inputs. \n",
    "    \n",
    "Higher F1 score better classifier.\n",
    "But more pcs don't mean better classifier, there is an optimal nof pcs that give best results.\n",
    "\n",
    "# Do not perform feature selection before PCA coz it'll throw information away. Feature selection\n",
    "\n",
    "can be performed after PCA to help improve model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA - Linear Discriminant Analysis - supervised dimensionality reduction\n",
    "the goal in LDA is to find the feature subspace that optimizes class separability.\n",
    "\n",
    "### LDA AND PCA\n",
    "\n",
    "Both LDA and PCA are linear transformation techniques that can be used to reduce the number of dimensions in a dataset; the former is an unsupervised algorithm, whereas the latter is supervised. Thus, we might intuitively think that LDA is a superior feature extraction technique for classification tasks compared to PCA. However, A.M. Martinez reported that preprocessing via PCA tends to result in better classification results in an image recognition task in certain cases, for instance, if each class consists of only a small number of samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA and PCA are both linear transformation techniques. For real world non linear data transformation, \n",
    "\n",
    "# Kernel PCA for non linear mappings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we can tackle nonlinear problems by projecting them onto a new feature space of higher dimensionality where the classes become linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "via kernel PCA we perform a nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the samples can be separated by a linear classifier (under the condition that the samples can be separated by density in the input space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one downside of this approach is that it is computationally very expensive, and this is where we use the kernel trick. Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we obtain after kernel PCA are the samples already projected onto the respective components rather than constructing a transformation matrix as in the standard PCA approach. Basically, the kernel function (or simply kernel) can be understood as a function that calculates a dot product between two vectors—a measure of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pipeline object takes a list of tuples as input, where the first value in each tuple is an arbitrary identifier string that we can use to access the individual elements in the pipeline, and the second element in every tuple is a scikit-learn transformer or estimator.\n",
    "\n",
    "The intermediate steps in a pipeline constitute scikit-learn transformers, and the last step is an estimator.\n",
    "\n",
    "![Pipeline](working_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "obtain reliable estimates of the model's generalization error, that is, how well the model performs on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout\n",
    "\n",
    "Data -> Train set, Test set\n",
    "\n",
    "Train set -> Train set, Validation set\n",
    "\n",
    "Training is done on training set, parameter tuning using validation set\n",
    "\n",
    "and performance evaluation using test set. \n",
    "\n",
    "A disadvantage of the holdout method is that the performance estimate is sensitive to how we partition the training set into the training and validation subsets; the estimate will vary for different samples of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Holdout](holdoutcv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "### Train Test split:\n",
    "\n",
    "splitting data into training and testing sets and using only training set for training and testing set to evaluate the model.\n",
    "\n",
    "1. Serves as a check on overfitting.\n",
    "2. Gives an estimate of performance on independent set.\n",
    "\n",
    "## Flow for split,pca, model training and prediction\n",
    "\n",
    "![title](train_test_pca_svm_flow.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k-fold cross-validation, we randomly split the training dataset into k folds without replacement, where k-1\n",
    "folds are used for the model training and one fold is used\n",
    "for testing. This procedure is repeated k times so that we obtain k models and performance estimates.\n",
    "\n",
    "We then calculate the average performance of the models based on the different, independent folds to obtain a performance estimate that is less sensitive to the subpartitioning of the training data compared to the holdout method\n",
    "\n",
    "Since k-fold cross-validation is a resampling technique without replacement, the advantage of this approach is that each sample point will be part of a training and test dataset exactly once, which yields a lower-variance estimate of the model performance than the holdout method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold cv tips: (default number of folds: 10)\n",
    "    \n",
    "1. Small training data: Increase the number of folds\n",
    "2. Large training data: Decrease the number of folds\n",
    "    \n",
    "Remember Large values of K Lower bias and Higher Variance\n",
    "         Small values of K Higher bias and Lower Variance.\n",
    "         \n",
    "Leave one out(LOO) used for very small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-fold\n",
    "\n",
    "stratified k-fold cross-validation can yield better bias and variance estimates, especially in cases of unequal class proportions.\n",
    "\n",
    "In stratified cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset.\n",
    " \n",
    "# Kfold\n",
    "\n",
    "Dividing the dataset into k subsets, taking each subset as a testing set once and remaining as training set and reporting the average of performance on K subsets. \n",
    "\n",
    "1. Slower to train than train/test split.\n",
    "2. Better estimate of model accuracy than train/test split.\n",
    "\n",
    "Just splits the data irrespective of classes coming in the train/test. This might result into training the model on one class and using it to predict the other which will as we expect perform poorly.\n",
    "\n",
    "## Training data should be such that it has a similar presence of all the classes as in the complete data set. \n",
    "\n",
    "# Stratified K-fold ensures that \n",
    "\n",
    "each set contains approximately the same percentage of samples of each target class as the complete set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learning and Validation Curves\n",
    "If a model is too complex for a given training dataset—there are too many degrees of freedom or parameters in this model—the model tends to overfit the training data and does not generalize well to unseen data. Often, it can help to collect more training samples to reduce the degree of overfitting. However, in practice, it can often be very expensive or simply not feasible to collect more data.\n",
    "\n",
    "\n",
    "High Bias model: low training accuracy, low cross-validation accuracy\n",
    "Steps to reduce bias(underfitting){Basically try to make a more complex model}:\n",
    "1. Add more features(create, add)\n",
    "2. Decrease degree of regularization\n",
    "\n",
    "High Variance model: High training accuracy, low cross-validation accuracy\n",
    "Steps to reduce Variance(overfitting){Basically try to make a simpler model}:\n",
    "1. If possible collect more data. (Be careful with more data, more noise might also come)\n",
    "2. Increase regularization strength.\n",
    "3. Decrease number of features via feature selection/extraction. \n",
    "\n",
    "Learning Curve: Plotting training and validation accuracies for different training set sizes.\n",
    "Validation Curve: Plotting training and validation accuracies for different values of model parameters. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "The goal behind ensemble methods is to combine different classifiers into a meta-classifier that has a better generalization performance than each individual classifier alone.\n",
    "\n",
    "## Majority and Plurality\n",
    "\n",
    "Majority voting simply means that we select the class label that has been predicted by the majority of classifiers, that is, received more than 50 percent of the votes. Strictly speaking, the term majority vote refers to binary class settings only. However, it is easy to generalize the majority voting principle to multi-class settings, which is called plurality voting. Here, we select the class label that received the most votes (mode).\n",
    "\n",
    "## Building Ensembles\n",
    "\n",
    "Depending on the technique, the ensemble can be built from different classification algorithms, for example, decision trees, support vector machines, logistic regression classifiers, and so on. \n",
    "\n",
    "Alternatively, we can also use the same base classification algorithm fitting different subsets of the training set. One prominent example of this approach would be the random forest algorithm, which combines different decision tree classifiers. The following diagram illustrates the concept of a general ensemble approach using majority voting.\n",
    "\n",
    "## Ensemble Error Probability\n",
    "\n",
    "When more than half of the base models predict wrongly.\n",
    "\n",
    "we make the assumption that all n base classifiers for a binary classification task have an equal error rate ε. Furthermore, we assume that the classifiers are independent and the error rates are not correlated. Under those assumptions, we can simply express the error probability of an ensemble of base classifiers as a probability mass function of a binomial distribution:\n",
    "\n",
    "\n",
    "![Ensemble Error](ensemble_error.png)\n",
    "\n",
    "## Learning: \n",
    "\n",
    "### if base error is high(0.40), more classifiers(250) would be required to build a good ensemble model.\n",
    "\n",
    "### If base error is low(0.25), less classifiers(40) would be required to build a good ensemble model.\n",
    "\n",
    "### Ensemble error is more than base error if base model performs worse than random guessing i.e 0.5 base error. \n",
    "\n",
    "\n",
    "# it is a bad practice to use the test dataset more than once for model evaluation, \n",
    "\n",
    "*Using it again and improving model according results makes test data part of the training data and then using the same test data to get an idea of generalization accuracy of the model will give false inflated results. \n",
    "\n",
    "# Bagging\n",
    "\n",
    "Sampling done with replacement to get different training sets. \n",
    "\n",
    "A base algo trained on different training sets giving different models. Voting used to get the prediction.\n",
    "\n",
    "Training set T: (1,2,3,4)\n",
    "\n",
    "T1: (1,3,3,4) -> Classifier 1,C1\n",
    "\n",
    "T2: (2,2,2,3) -> Classifier 2,C2\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "etc\n",
    "\n",
    "# Bagging does not reduce bias. Bagging reduces variance.\n",
    "\n",
    "\n",
    "# Adaptive Boosting: Weak Learners to a Strong model\n",
    "\n",
    "Weak Learners \n",
    "\n",
    "The key concept behind boosting is to focus on training samples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training samples to improve the performance of the ensemble. In contrast to bagging, the initial formulation of boosting, the algorithm uses random subsets of training samples drawn from the training dataset without replacement. The original boosting procedure is summarized in four key steps as follows:\n",
    "\n",
    "Boosting rounds\n",
    "\n",
    "First round:\n",
    "\n",
    "1.Uniform weights initialized.\n",
    "\n",
    "2.error calculated(true, predicted)\n",
    "\n",
    "3.coefficient alpha calculated\n",
    "\n",
    "4.weights modified\n",
    "\n",
    "5.weights normalized\n",
    "\n",
    "Next round from step 2\n",
    "\n",
    "# Adaptive boosting and boosting in general add variance but help reducing bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "# Bag of words \n",
    "\n",
    "Converting text to numerical data.\n",
    "\n",
    "          word1 word2 word3 word4.....wordn\n",
    "       text1  1     1    0      4         3\n",
    "       text2  0     2    1      0         1\n",
    "       \n",
    "Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will consist of mostly zeros, which is why we call them sparse\n",
    "\n",
    "# N grams\n",
    "\n",
    "• 1-gram: \"the\", \"sun\", \"is\", \"shining\" \n",
    "\n",
    "• 2-gram: \"the sun\", \"sun is\", \"is shining\"\n",
    "\n",
    "the contiguous sequences of items in NLP—words, letters, or symbols—is also called an n-gram. The choice of the number n in the n-gram model depends on the particular application; for example, a study by Kanaris et al. revealed that n-grams of size 3 and 4 yield good performances in anti-spam filtering of e-mail messages\n",
    "\n",
    "The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with ngram_range=(2,2).\n",
    "\n",
    "## term frequencies(tf): Number of times term t occurs in document d. tf(t, d)\n",
    "## inverse document frequency(idf):\n",
    "![Idf](idf.png)\n",
    "\n",
    "nd = total number of documents\n",
    "\n",
    "df(d,t) = number of documents that contain term t\n",
    "\n",
    "![tf-idf](tfidf.png)\n",
    "\n",
    "Sklearns TfidfTransformer applies L2 normalization after calculating tfidfs\n",
    "\n",
    "# Processing document into tokens\n",
    "\n",
    "    Splitting: splitting text by whitespaces\n",
    "    Word Stemming: transforming words to their roots \n",
    "    \n",
    "# Training heavy data sets takes a lot of time on a local machine.\n",
    "\n",
    "# Out of core learning helps tackle large datasets\n",
    "\n",
    "### we can't use the CountVectorizer for out-of-core learning since it requires holding the complete vocabulary in memory. Also, the TfidfVectorizer needs to keep the all feature vectors of the training dataset in memory to calculate the inverse document frequencies. However, another useful vectorizer for text processing implemented in scikit-learn is HashingVectorizer. HashingVectorizer is data-independent and makes use of the Hashing trick via the 32-bit MurmurHash3 algorithm by Austin Appleby\n",
    "\n",
    "\n",
    "# Learning From Text\n",
    "\n",
    "Bag of Words: Frequency counts of occuring words.\n",
    "\n",
    "Using sklearn countvectorizer\n",
    "\n",
    "### All words are not equally important, words like the/a/an/is/etc don't tell much about what's going on and so are redundant called \"STOPWORDS\" \n",
    "\n",
    "Remove stopwords before starting text analysis.\n",
    "\n",
    "STEMMER: used to consolidate different words with same stem like repond, responsiveness etc.\n",
    "various stemmers in nltk eg. snowball stemmer and more.\n",
    "\n",
    "## Order of operation in Text processing\n",
    "\n",
    "1. Stop words removal\n",
    "2. Stemming\n",
    "3. Bag of words\n",
    "\n",
    "# TF IDF : \n",
    "Term Frequency: How many times a word occurs in a document.\n",
    "Inverse Document Frequency: In how many document a word occurs.\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "1. Select best features\n",
    "2. Engineer new features\n",
    "3. Getting Rid of features\n",
    "\n",
    "## Engineering new Feature:\n",
    "\n",
    "1. Use Human Intuition\n",
    "2. Code up the feature\n",
    "3. Visualize : See if there are trends which can be utilized by ML algos. \n",
    "4. Repeat\n",
    "\n",
    "### Beware of programming bugs that might creep in while engineering new features.\n",
    "\n",
    "1. Anyone can make mistakes--be skeptical of your results!\n",
    "2. 100% accuracy should generally make you suspicious. Extraordinary claims require extraordinary proof.\n",
    "3. If there's a feature that tracks your labels a little too closely, it's very likely a bug!\n",
    "4. If you're sure it's not a bug, you probably don't need machine learning--you can just use that feature alone to assign labels.\n",
    "\n",
    "## Getting Rid of features\n",
    "\n",
    "Remove the feature when:\n",
    "\n",
    "1. It's noisy\n",
    "2. It's highly correlated to other feature. (Repeating information)\n",
    "3. It causes overfitting\n",
    "4. slows down training/testing\n",
    "\n",
    "## General Rule\n",
    "\n",
    "# Features are not equal to information. Features attempt to access information.\n",
    "\n",
    "# Goal: Bare minimum number of features that give the most info.\n",
    "\n",
    "# Univariate Feature Selection:\n",
    "Treats each feature independently and asks how much power it gives you in classifying or regressing.\n",
    "\n",
    "sklearn:\n",
    "\n",
    "1. SelectPercentile: X% of features that are most powerful \n",
    "2. SelectKBest: selects the K features that are most powerful\n",
    "3. TFIDF vectorizer max_df, min_df can also help get the right features. \n",
    "\n",
    "Text data has lots and lots of features , feature reduction can be used.\n",
    "Feature reduction can be used for highly dimensional data. \n",
    "\n",
    "# A classic way to overfit an algorithm is by using lots of features and not a lot of training data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying on Web\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Univariate: one independent(explanatory) variable\n",
    "\n",
    "Bivariate: two independent variables\n",
    "\n",
    "Multivariate: Many independent variables\n",
    "\n",
    "X =  explanatory variable\n",
    "\n",
    "y =  response variable\n",
    "    \n",
    "    \n",
    "## linear regression can be understood as finding the best-fitting straight line through the sample points\n",
    "\n",
    "This best-fitting line is also called the regression line, and the vertical lines from the regression line to the sample points are the so-called offsets or residuals—the errors of our prediction.\n",
    "\n",
    "# Exploration | Visualization\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an important and recommended first step prior to the training of a machine learning model.\n",
    "\n",
    "1. Scatter plots, histograms\n",
    "\n",
    "# Correlation Matrix\n",
    "\n",
    "The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients (often abbreviated as Pearson's r), which measure the linear dependence between pairs of features.\n",
    "\n",
    "The correlation coefficients are bounded to the range -1 and 1.\n",
    "\n",
    "r = 1  --> Perfectly positive correlation.\n",
    "r = 0  --> No correlation.\n",
    "r = -1 --> Perfectly Negative correlation.\n",
    "\n",
    "Pearson's correlation coefficient can simply be calculated as the covariance between two features x and y (numerator) divided by the product of their standard deviations (denominator):\n",
    "\n",
    "![Pearson coefficient](pearson_coefficient.png)\n",
    "\n",
    "# To fit a linear regression model, we are interested in those features that have a high correlation with our target variable MEDV\n",
    "\n",
    "Highest correlations of target variable MEDV are with LSTAT, RM.\n",
    "\n",
    "Looking at plots it can be observed that LSTAT has a fairly non linear relationship with MEDV. But MEDV shows a linear relationship with RM.\n",
    "\n",
    "\n",
    "# Fitting a Robust Regression Model\n",
    "\n",
    "As an alternative to throwing out outliers, we will look at a robust method of regression using the RANdom SAmple Consensus (RANSAC) algorithm, which fits a regression model to a subset of the data, the so-called inliers. Using the residual_metric parameter, we provided a callable lambda function that simply calculates the absolute vertical distances between the fitted line and the sample points. By setting the residual_threshold parameter to 5.0, we only allowed samples to be included in the inlier set if their vertical distance to the fitted line is within 5 distance units, which works well on this particular dataset. By default, scikit-learn uses the MAD estimate to select the inlier threshold, where MAD stands for the Median Absolute Deviation of the target values y. However, the choice of an appropriate value for the inlier threshold is problem-specific, which is one disadvantage of RANSAC.\n",
    "\n",
    "# Residual Plots\n",
    "\n",
    "we can plot the residuals (the differences or vertical distances between the actual and predicted values) versus the predicted values to diagnose our regression model. Those residual plots are a commonly used graphical analysis for diagnosing regression models to detect nonlinearity and outliers, and to check if the errors are randomly distributed.\n",
    "\n",
    "In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect that the errors are randomly distributed and the residuals should be randomly scattered around the centerline. If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information, which is leaked into the residuals as we can slightly see in our preceding residual plot. Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline\n",
    "\n",
    "# Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE), which is simply the average value of the SSE cost function that we minimize to fit the linear regression model. The MSE is useful to for comparing different regression models or for tuning their parameters via a grid search and cross-validation:\n",
    "\n",
    "As we saw above MSE test is more than MSE train which means model is certainly overfitting the data. \n",
    "\n",
    "MSE order is dependent on y. A standardized version of MSE is R^2(R squared) which is \n",
    "\n",
    "R^2 = 1 - (SSE/SST)  =  1 - (MSE/var(y))\n",
    "\n",
    "SSE  = sum((y(i) - ypred(i))^2)\n",
    "\n",
    "\n",
    "SST  = sum((y(i) - mean(y))^2) \n",
    "\n",
    "# Regularized Regression Models\n",
    "\n",
    "1. Ridge regression: Quadratic Penalty | L2 penalty : add lambda*sum(w^2) to the SSE(cost function)\n",
    "\n",
    "2. Lasso regression: Linear Penalty | L1 penalty : add lambda*sum(w) to the SSE(cost function)\n",
    "\n",
    "Depending on the regularization strength, certain weights can become zero, which makes the LASSO also useful as a supervised feature selection technique:\n",
    "\n",
    "3. Elastic Net: combination of ridge and lasso  | add L1 + L2 terms to cost function.\n",
    "\n",
    "As lambda increases regularization strength increases and weights shrink.\n",
    "Intercept term is not regularized.\n",
    "\n",
    "# Important tips and conclusion\n",
    "\n",
    "As we can see cubic fits better than linear and quadratic. As degree increases training accuracy increases but it makes model more complex which may result into overfitting. \n",
    "\n",
    "Non linear problems can be dealt with:\n",
    "\n",
    "1. Polynomial regression\n",
    "2. Transforming variables - log, sqrt, cube root etc\n",
    "3. Decision tree and Random forests, SVMs\n",
    "     In the context of decision tree regression, the MSE is often also referred to as within-node variance, which is why the splitting criterion is also better known as variance reduction\n",
    "     \n",
    "     A random forest usually has a better generalization performance than an individual decision tree due to randomness that helps to decrease the model variance. Other advantages of random forests are that they are less sensitive to outliers in the dataset and don't require much parameter tuning. The only parameter in random forests that we typically need to experiment with is the number of trees in the ensemble.\n",
    "     \n",
    "     The only difference is that we use the MSE criterion to grow the individual decision trees, and the predicted target variable is calculated as the average prediction over all decision trees.\n",
    "     \n",
    "# Regression (Continuous output)\n",
    "\n",
    "Minimizes sum of squared errors (actual - predicted). \n",
    "Finds slope and intercept for the line which minimizes sum of squared errors.\n",
    "\n",
    "absolute error minimization not used because it can give us more than one lines. \n",
    "\n",
    "In case of squared errors there will be only one line. also SSE is easier to implement. \n",
    "## Problem with SSE:\n",
    "1. Adding more data increases SSE but that doesn't mean fit is bad.\n",
    "\n",
    "\n",
    "This is done by:\n",
    "\n",
    "Ordinary Least squares OLS (used in sklearn)\n",
    "\n",
    "Linear descent\n",
    "\n",
    "## Performance Measure for Regression : R-Squared\n",
    "\n",
    "R-Squared: \"How much of change in the output is explained by the change in the input.\n",
    "\n",
    " 0.0 < R-Squared < 1.0 (Best)\n",
    " \n",
    "Negative R-Squared is possible.*\n",
    "\n",
    "Advantage over SSE:\n",
    "1. Independent of datapoints. \n",
    "Higher the R-Squared, the better. Max value = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "# K means clustering\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Assign: Randomly assign cluster centers.\n",
    "2. Cluster Identification: Find points nearest to these cluster centers to identify the clusters. \n",
    "3. Centroid :Find the centroid of these clusters. New cluster centers are these centroids.\n",
    "4. Repeat 2 & 3 until cluster centers stop updating.\n",
    "\n",
    "\n",
    "sklearn params:\n",
    "\n",
    "n_clusters = Number of clusters we want to have.\n",
    "n_init = How many times it is initialized. Play with this if you see clustering getting affected by initialization. \n",
    "\n",
    "max_iter = How many iterations in total?\n",
    "\n",
    "\n",
    "Limitations :\n",
    "\n",
    "1. Premature convergence to sub optimal values. \n",
    "2. Can result into different clusters based on Initialization.\n",
    "\n",
    "business-oriented applications of clustering include the grouping of documents, music, and movies by different topics, or finding customers that share similar interests based on common purchase behaviors as a basis for recommendation engines.\n",
    "\n",
    "## Types of clustering:\n",
    "\n",
    "1. Prototype based: Centroid(average for continuous vars) , Medoid(most occuring point for categorical features) eg. Kmeans\n",
    "\n",
    "2. Hierarchical \n",
    "\n",
    "3. Density based \n",
    "\n",
    "Kmeans:\n",
    "    \n",
    "Advantages: simple, easy to implement and commputationally efficient\n",
    "    \n",
    "Disadvantages: Have to define K apriori, wrong choice of K might result in bad clustering. Another problem with k-means is that one or more clusters can be empty(addressed in scikit learn- assigning the farthest point from centroid of empty cluster as the new centroid)\n",
    "\n",
    "# Standardization/Scaling(features have to be on the same scale) required when using Euclidean distance\n",
    "\n",
    "# Hard Clustering: Each sample asssigned to only one cluster. KMeans\n",
    "\n",
    "# Soft Clustering: A sample is assigned to one or more clusters. FCM(fuzz C means)\n",
    "\n",
    "each sample has a probability for different clusters.\n",
    "\n",
    "# Optimum number of clusters\n",
    "\n",
    "1. Distortion(withing cluster SSE) (Inertia attribute of KMeans): How far are the samples from centroid.\n",
    "\n",
    "if K(nof clusters) increases distortion decreases as points are closer to centroids. \n",
    "\n",
    "## Finding the K where distortion shoots up - \"Look for the elbow in plot\"\n",
    "\n",
    "# Silhouette Analysis: Quality of clustering\n",
    "\n",
    "Applicable to other clustering algorithms too.\n",
    "\n",
    "Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the samples in the clusters are.\n",
    "\n",
    "Silhouette Coefficient\n",
    "![Silhoutte_coeff](silhoutte_coeff.png)\n",
    "\n",
    "ai = average distance of sample xi from the samples of the cluster | quantifies how similar a sample is to other samples of the cluster\n",
    "\n",
    "bi = average distance of sample xi from the samples of the nearest cluster | quantifies how dissimilar a sample is from nearest cluster samples\n",
    "\n",
    "Silhouette coefficient = 1 Ideal silhoutte | b >> a\n",
    "\n",
    "silhouette coefficient range (-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation Metric\n",
    "\n",
    "True Positive: When we predicted positive and actual value is positive.\n",
    "True Negative: When we predicted negative and actual value is negative.\n",
    "\n",
    "False Positive: When we predicted positive and actual value is negative. \n",
    "False negative: When we predicted negative and actual value is positive. \n",
    "\n",
    "## Accuracy and Error\n",
    "\n",
    "Accuracy: Correct predictions/Total predictions\n",
    "\n",
    "Accuracy =  (TP+ TN)/(TP+TN+FP+FN)\n",
    "\n",
    "Error = Wrong predictions/Total predictions\n",
    "\n",
    "Error = (FP+FN)/(TP+TN+FP+FN))\n",
    "\n",
    "Accuracy = 1  -  Error\n",
    "\n",
    "## TPR and FPR\n",
    "\n",
    "The true positive rate (TPR) and false positive rate (FPR) are performance metrics that are especially useful for imbalanced class problems:\n",
    "\n",
    "FPR =  FP/N = FP/(FP+TN)\n",
    "\n",
    "FPR = How many times we incorrectly predicted positive when actual value was negative. \n",
    "\n",
    "TPR = TP/P = TP/(TP+FN) = How many times we correctly predicted positive when actual value was postive. \n",
    "\n",
    "## Precision and Recall\n",
    "\n",
    "Precision (PRE) and recall (REC) are performance metrics that are related to those true positive and true negative rates, and in fact, recall is synonymous to the true positive rate\n",
    "\n",
    "PRE = TP/(TP+FP) , how many times we are correct when we predicted positive. \n",
    "\n",
    "Recall = TP/(TP+FN) : Same as True positive rate.\n",
    "\n",
    "F1-score = 2*(PRExREC)/(PRE+REC)\n",
    "\n",
    "## Remember that the positive class in scikit-learn is the class that is labeled as class 1. If we want to specify a different positive label, we can construct our own scorer via the make_scorer function, which we can then directly provide as an argument to the scoring parameter in GridSearchCV:\n",
    "\n",
    "# ROC - Receiver Operating Characteristic curves\n",
    "\n",
    "Receiver operator characteristic (ROC) graphs are useful tools for selecting models for classification based on their performance with respect to the false positive and true positive rates, which are computed by shifting the decision threshold of the classifier.\n",
    "\n",
    "The diagonal of an ROC graph can be interpreted as random guessing, and classification models that fall below the diagonal are considered as worse than random guessing. A perfect classifier would fall into the top-left corner of the graph with a true positive rate of 1 and a false positive rate of 0. Based on the ROC curve, we can then compute the so-called area under the curve (AUC) to characterize the performance of a classification model.\n",
    "\n",
    "## Macro average for multiclass problems\n",
    "\n",
    "learn, a normalized or weighted variant of the macro-average is used by default. The weighted macro-average is calculated by weighting the score of each class label by the number of true instances when calculating the average. The weighted macro-average is useful if we are dealing with class imbalances, that is, different numbers of instances for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

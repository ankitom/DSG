{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function: It takes in z = w*x and if actf(z)> threshold(theta) then 1 and -1 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for perceptron activation function is unit step, i.e. actf(z) = 1 if z>theta and actfz(z) = -1 if z < theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Perceptron rule:   w_change = ita*(y_true - y_pred)*x\n",
    "After w_change is calculated , weights are updated w_new = w_change + w_old\n",
    "where ita is learning rate generally between 0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the convergence of the perceptron is only guaranteed if the two classes are linearly separable and the learning rate is sufficiently small otherwise weights keep getting updated. To avoid this , run the perceptron for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ADALINE: Adaptive Linear Neuron\n",
    "\n",
    "Uses identity function as activation function.  actf(w_transpo*x) = w_transpo*x\n",
    "\n",
    "and a step function for classification (quantizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the case of Adaline, we can define the cost function J to learn the weights as the Sum of Squared Errors (SSE) between the calculated outcome and the true class label.\n",
    "\n",
    "J is differentiable and convex thus \"Gradient descent\" a simple yet powerful optimization algo can be\n",
    "used to get weights minimizing the cost function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choosing a Classification algo\n",
    "\n",
    "\"No Free Lunch\" theorem: no single classifier works best across all possible scenarios. In practice, \n",
    "\n",
    "### it is always recommended that you compare the performance of at least a handful of different learning algorithms to select the best model for the particular problem; \n",
    "\n",
    "these may differ in the \n",
    "\n",
    "1. number of features or samples,\n",
    "\n",
    "2. the amount of noise in a dataset, and \n",
    "\n",
    "3. whether the classes are linearly separable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Steps to solve a Machine learning problem:\n",
    " \n",
    " 1. Selection of features. \n",
    " \n",
    " 2. Choosing a performance metric. \n",
    " \n",
    " 3. Choosing a classifier and optimization algorithm. \n",
    " \n",
    " 4. Evaluating the performance of the model. \n",
    " \n",
    " 5. Tuning the algorithm.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling features\n",
    "\n",
    "##  Use the same scaling parameters to standardize the test set so that both the values in the training and test dataset are comparable to each other. Use the same scaler that was fit using training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "It is a classification algorithm despite the name. \n",
    "\n",
    "A linear model, used for binary classification, can be extended to multiclass classificatioin using OVR(one vs rest) technique.\n",
    "\n",
    "Probabalistic model:\n",
    "\n",
    "Odds ratio = p/(1-p) , \n",
    "where p is probability of the even happening. \n",
    "\n",
    "Logistic regression is used in weather forecasting, for example, to not only predict if it will rain on a particular day but also to report the chance of rain. Similarly, logistic regression can be used to predict the chance that a patient has a particular disease given certain symptoms, which is why logistic regression enjoys wide popularity in the field of medicine.\n",
    "\n",
    "\n",
    "logit(p) = log(p/(1-p))\n",
    "\n",
    "z = w_transpose . x(input)\n",
    "\n",
    "f(z) = Logistic function = 1/(1 + e^(-z)) = Probability of event occuring = p\n",
    "\n",
    "1/(1+e^(-z)) is called sigmoid function because of the s shape it has.\n",
    "\n",
    "for given weights and x, z = w_transpose. x can be found and using sigmoid function and z probability is found. \n",
    "\n",
    "Logistic regression can be used to predict probabilities and class labels.\n",
    "\n",
    "For updating weights\n",
    "### Maximizing the log likelihood = Minimizing the cost function \n",
    "\n",
    "## ∆w = −eta. ∇J(w )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Overfitting: High Variance: Can't generalize \n",
    "\n",
    "Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data).\n",
    "\n",
    "If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters that lead to a model that is too complex given the underlying data. \n",
    "\n",
    "# Underfitting: High Bias: Highly susceptible to data. \n",
    "\n",
    "means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.\n",
    "\n",
    "## Variance: Very Different results for different dataset\n",
    "\n",
    "## Bias: Far from correct values. \n",
    "\n",
    "# Tackling overfitting via regularization\n",
    "\n",
    "Regularization is a very useful method to handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights. The most common form of regularization is the so-called L2 regularization (sometimes also called L2 shrinkage or weight decay\n",
    "\n",
    "# C = 1/λ\n",
    "\n",
    "where, \n",
    "λ = Regularization Strength = More the lambda lesser the weights.\n",
    "C  = Inverse regularization parameter (in Logistic Regression sklearn).\n",
    "By tuning C we can control regularization of in Logistic Regression. \n",
    "\n",
    "# More the C more the weights.\n",
    "# As C decreases weights shrink.\n",
    "\n",
    "# L1 Regularization: linear sum(|w|) --> encourages sparsity, diamond shaped.\n",
    "# L2 Regularization: quadratic sum(w^2) --> (circular)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Feature scaling\n",
    "\n",
    "Regularization is another reason why feature scaling such as standardization is important. For regularization to work properly, we need to ensure that all our features are on comparable scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Support Vector Machines\n",
    "\n",
    "Finding a HyPlane(plane separating classes) which is at max distance from nearest samples(support vectors)\n",
    "Maximizing margin between Hyperplane and support vectors(samples closest to HyPlane)\n",
    "\n",
    "Bias-Variance can be controlled by C : Higher the C more penalty for misclassifications lesser variance and more the bias.\n",
    "### Lower C lesser penalties softer margins.\n",
    "\n",
    "Kernel trick: Can be used to tap into high dimensional space to find a linear separating plane in the high dimension where the data seems non linear in regular dimension. Projecting non linear combination on features to higher dimensional space. \n",
    "\n",
    "Roughly speaking, the term kernel can be interpreted as a similarity function between a pair of samples.\n",
    "\n",
    "Gamma = Influence on a sample on nearby region. More the gamma less the influence.\n",
    "\n",
    "## SVMs are hard to scale,\n",
    "\n",
    "## SGDClassifier can be used for efficient scaling to very large datasets. \n",
    "\n",
    "Different algos from SGDClassifier.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "ppn = SGDClassifier(loss='perceptron') \n",
    "\n",
    "lr = SGDClassifier(loss='log') \n",
    "\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "\n",
    "http://scikit-learn.org/stable/modules/sgd.html \n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "Maximizing information gain at each node. \n",
    "\n",
    "Commonly used splitting criteria(Measures of impurity):\n",
    "\n",
    "1. Gini index -  Ig = 1 - ∑p(i/t)\n",
    "\n",
    "2. Entropy - Ie = -∑p(i/t)log<sub>2</sub>(p(i/t)\n",
    "\n",
    "3. Classification error - Ie =  1 - max(p(i/t))\n",
    "\n",
    "Here, (p(i/t) is the proportion of the samples that belongs to class c for a particular node t. \n",
    "\n",
    "in practice both the Gini index and entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.\n",
    "\n",
    "Classification error is useful for pruning but not for growing tree. \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Forests -  Combining weak learners to Strong ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Randomly choose n samples from training set.\n",
    "2. Grow a decision tree select d random features at each node.\n",
    "3. Repeat steps 1 and 2 , k times.\n",
    "4. Aggregate the prediction by each tree to assign a class label through majority vote.\n",
    "\n",
    "A resonable default for  d is sqrt(m) where m is nof features in training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind ensemble learning is to combine weak learners to build a more robust model, a strong learner, that has a better generalization error and is less susceptible to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a big advantage of random forests is that we don't have to worry so much about choosing good hyperparameter values. We typically don't need to prune the random forest since the ensemble model is quite robust to noise from the individual decision trees. The only parameter that we really need to care about in practice is the number of trees k (step 3) that we choose for the random forest. Typically, the larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost. No need to standardize or normalize tree based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors\n",
    "\n",
    "Finds k closest neighbors using a distance metric e.g euclidean distance, manhattan distance and assigns class label by majority vote.  KNN learns training data. Simple but requires lot of memory and not scalable. Complexity grows linearly. \n",
    "\n",
    "## The curse of dimensionality\n",
    "It is important to mention that KNN is very susceptible to overfitting due to the curse of dimensionality. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset. Intuitively, we can think of even the closest neighbors being too far away in a high-dimensional space to give a good estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of classification algorithms\n",
    "\n",
    "We have seen that decision trees are particularly attractive if we care about interpretability. Logistic regression is not only a useful model for online learning via stochastic gradient descent, but also allows us to predict the probability of a particular event. Although support vector machines are powerful linear models that can be extended to nonlinear problems via the kernel trick, they have many parameters that have to be tuned in order to make good predictions. In contrast, ensemble methods such as random forests don't require much parameter tuning and don't overfit so easily as decision trees, which makes it an attractive model for many practical problem domains. The K-nearest neighbor classifier offers an alternative approach to classification via lazy learning that allows us to make predictions without any model training but with a more computationally expensive prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn. Therefore, it is absolutely critical that we make sure to examine and preprocess a dataset before we feed it to a learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although scikit-learn was developed for working with NumPy arrays, it can sometimes be more convenient to preprocess data using pandas' DataFrame. We can always access the underlying NumPy array of the DataFrame via the values attribute before we feed it into a scikit-learn estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding sklearn estimator\n",
    "\n",
    "## Any data array that is to be transformed needs to have the same number of features as the data array that was used to fit the model.\n",
    "\n",
    "## The fit method is used to learn the parameters from the training data, and the transform method uses those parameters to transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sklearn Estimator](sklearnestimator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Handling missing values \n",
    "\n",
    "1. dropping\n",
    "2. Imputing\n",
    "\n",
    "## Handling categorical data\n",
    "\n",
    "1. LabelEncoding class labels.\n",
    "2. Mapping nominal features to numbers \n",
    "3. One hot encoding nominal features - Getting dummies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training and testing sets\n",
    "\n",
    "## Bringing features onto the same scale.\n",
    "\n",
    "Normalization: bringing data between [0,1]\n",
    "\n",
    "Standardization: standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting \n",
    "\n",
    "occurs when our model performs very well on training set but fails to genralize well on testing sets.  It means our model is too complex for the given data. Overfitting can be reduced by:\n",
    "\n",
    "1. Using less but important features\n",
    "2. Collecting more training data\n",
    "3. Introduce a penalty for complexity via regularization (explained above)\n",
    "4. Reduce dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "There are two main categories of dimensionality reduction techniques: feature selection and feature extraction.\n",
    "\n",
    "Feature selection: we select a subset of the original features. \n",
    "Feature extraction: we derive information from the feature set to construct a new feature subspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Feature Selection \n",
    "\n",
    "Feature selection: selecting a subset of original features. \n",
    "\n",
    "The motivation behind feature selection algorithms is to automatically select a subset of features that are most relevant to the problem to improve computational efficiency or reduce the generalization error of the model by removing irrelevant features or noise, which can be useful for algorithms that don't support regularization.\n",
    "\n",
    "## Sequential Backward Selection(SBS)\n",
    "\n",
    "Removing features in stages, at each stage a feature that causes minimal accuracy loss (accuracy_before - accuracy_after) is removed. \n",
    "\n",
    "# Feature removal throught regularization (l1 and l2)\n",
    "\n",
    "Penalizing weights. \n",
    "\n",
    "# Feature Importances \n",
    "\n",
    "## Random forest feature importance\n",
    "\n",
    "feature importance as the averaged impurity decrease computed from all decision trees in the forest without making any assumptions whether our data is linearly separable or not. we don't need to use standardized or normalized tree-based models\n",
    "\n",
    "scikit-learn also implements a transform method that selects features based on a user-specified threshold after model fitting, which is useful if we want to use the RandomForestClassifier as a feature selector and intermediate step in a scikit-learn pipeline, which allows us to connect different preprocessing steps with an estimator, as we will see in Chapter 6, Learning Best Practices for Model Evaluation and Hyperparameter Tuning. For example, we could set the threshold to 0.15 to reduce the dataset to the 3 most important features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Feature extraction\n",
    "\n",
    "PCA: Principal component analysis\n",
    "Data is transformed into a low/equal dimensional feature subspace such that principal components(directions of maximum variance) are\n",
    "orthogonal to each other. \n",
    "\n",
    "PCA can be used for:\n",
    "1. Feature extraction(unsupervised learning)\n",
    "2. De noising of signals\n",
    "3. Exploratory data analysis\n",
    "4. Analysis of genome data and gene expression\n",
    "\n",
    "PCA helps us to identify patterns in data based on the correlation between features\n",
    "\n",
    "PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. The orthogonal axes (principal components) of the new subspace can be interpreted as the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other.\n",
    "\n",
    "![Principal components](pca.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Standardize features before PCA\n",
    "\n",
    "### Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Steps involved in PCA`\n",
    "\n",
    "1. Standardize the d -dimensional dataset. \n",
    "2. Construct the covariance matrix.  (The symmetric d d× -dimensional covariance matrix, where d is the number of dimensions in the dataset, stores the pairwise covariances between the different features.)\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Select k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k d≤ ).\n",
    "5. Construct a projection matrix W from the \"top\" k eigenvectors.\n",
    "6. Transform the d -dimensional input dataset X using the projection matrix W to obtain the new k -dimensional feature subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Covariance\n",
    "\n",
    "A positive covariance between two features indicates that the features increase or decrease together, whereas a negative covariance indicates that the features vary in opposite directions.\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude. \n",
    "\n",
    "### Eigen values and Eigen vectors\n",
    "\n",
    "From a NxN covariance matrix we get N eigen vectors, eigen values are their magnitudes. \n",
    "We only select the subset of the eigenvectors (principal components) that contains most of the information (variance).\n",
    "\n",
    "We are interested in the top k eigenvectors based on the values of their corresponding eigenvalues.\n",
    "\n",
    "The variance explained ratio of an eigenvalue λ is simply the fraction of an eigenvalue λ and the total sum of the eigenvalues.\n",
    "\n",
    "## PCA is an unsupervised method, which means that information about the class labels is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA - Linear Discriminant Analysis - supervised dimensionality reduction\n",
    "the goal in LDA is to find the feature subspace that optimizes class separability.\n",
    "\n",
    "### LDA AND PCA\n",
    "\n",
    "Both LDA and PCA are linear transformation techniques that can be used to reduce the number of dimensions in a dataset; the former is an unsupervised algorithm, whereas the latter is supervised. Thus, we might intuitively think that LDA is a superior feature extraction technique for classification tasks compared to PCA. However, A.M. Martinez reported that preprocessing via PCA tends to result in better classification results in an image recognition task in certain cases, for instance, if each class consists of only a small number of samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "LDA and PCA are both linear transformation techniques. For real world non linear data transformation, \n",
    "\n",
    "# Kernel PCA for non linear mappings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we can tackle nonlinear problems by projecting them onto a new feature space of higher dimensionality where the classes become linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "via kernel PCA we perform a nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the samples can be separated by a linear classifier (under the condition that the samples can be separated by density in the input space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one downside of this approach is that it is computationally very expensive, and this is where we use the kernel trick. Using the kernel trick, we can compute the similarity between two high-dimension feature vectors in the original feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we obtain after kernel PCA are the samples already projected onto the respective components rather than constructing a transformation matrix as in the standard PCA approach. Basically, the kernel function (or simply kernel) can be understood as a function that calculates a dot product between two vectors—a measure of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
